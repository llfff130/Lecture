以下是根据文件内容整理的Markdown笔记：

# HUST-USYD 并行编程实践暑期学校 —— 第 10 讲
## 大纲
- 回顾作业 6 —— 高斯消元法的部分主元并行算法
- 派生数据类型
- 实验室练习
- 总结
- 分布式内存机器上的并行算法设计 —— MPI 编程
- 作业 7

## 高斯消元法的部分主元
- **部分主元**：交换行，使 A(i,i) 是列中最大的值。
- **引理**：该算法计算 A = P*L*U，其中 P 是置换矩阵。
- **数值稳定性**：该算法在实践中数值稳定，是标准方法，但通信成本需要考虑。
- **算法流程**：
    - 对于 i = 1 到 n - 1：
        - 找到并记录 k，使得 |A(k,i)| = max{i ≤ j ≤ n} |A(j,i)|，即列 i 中剩余部分的最大值。
        - 如果 |A(k,i)| = 0，发出警告，表示 A 是奇异的，或者近乎奇异。
        - 否则如果 k ≠ i，交换 A 的第 i 行和第 k 行。
        - A(i+1:n,i) = A(i+1:n,i) / A(i,i)，每个 |商| ≤ 1。
        - A(i+1:n,i+1:n) = A(i+1:n,i+1:n) - A(i+1:n,i) * A(i,i+1:n)。

## 基本高斯消元法的问题
- 当前计算全部是 BLAS 1 或 BLAS 2，但我们知道 BLAS 3（矩阵乘法）最快。
- **算法流程**：
    - 对于 i = 1 到 n - 1：
        - A(i+1:n,i) = A(i+1:n,i) / A(i,i)……BLAS 1（缩放向量）。
        - A(i+1:n,i+1:n) = A(i+1:n,i+1:n)……BLAS 2（秩 1 更新）- A(i+1:n,i) * A(i,i+1:n)。
- **性能问题**：
    - BLAS 1 和 BLAS 2 的性能不如 BLAS 3，需要优化。

## 将 BLAS2 转换为 BLAS3
- **分块**：用于优化矩阵乘法。
- **延迟更新**：保存来自多个连续 BLAS2（秩 1 更新）的更新到“尾随矩阵”，然后同时应用多个更新到一个 BLAS3（矩阵乘法）操作。
- **算法流程**：
    - 对于 ib = 1 到 n - 1，步长为 b……一次处理 b 列。
    - 结束 = ib + b - 1……指向 b 列的结尾。
    - 应用 BLAS2 版本的 GEPP，得到 A(ib:n,ib:end) = P' * L' * U'。
    - 让 LL 表示 A(ib:end,ib:end) + I 的严格下三角部分。
    - A(ib:end,end+1:n) = LL^-1 * A(ib:end,end+1:n)……更新 U 的下一行。
    - A(end+1:n,end+1:n) = A(end+1:n,end+1:n) - A(end+1:n,ib:end) * A(ib:end,end+1:n)……应用延迟更新，使用单个矩阵乘法……内层维度为 b。
- **块大小选择**：
    - b 应该足够小，以便由 b 列组成的活动子矩阵能够放入缓存。
    - b 应该足够大，以使 BLAS3（矩阵乘法）快速。

## 任务/数据划分与分配
- **共享内存机器**：任务分配相对简单，因为全局数据不属于任何线程，给任务分配带来很大的灵活性。
- **分布式内存机器**：
    - 数据必须分布在各个进程之间。
    - 任务分配的灵活性较低，需要使用消息传递交换数据。
    - 必须考虑如何最小化通信成本。

## 并行高斯消元法的不同数据布局
- **1D 列分块布局**：负载不平衡，P0 在前 n/4 步后空闲。
- **1D 列循环布局**：负载平衡，但不能轻松使用 BLAS3。
- **1D 列块循环布局**：可以通过选择 b 来权衡负载平衡和 BLAS3 性能，但块列的分解是一个瓶颈。
- **块斜布局**：复杂的寻址，可能不想在每一列、行中实现完全并行。
- **2D 行和列分块布局**：负载不平衡，P0 在前 n/2 步后空闲。
- **2D 行和列块循环布局**：获胜者！

## 分布式内存机器上的 LU 算法
- **算法流程**：（具体内容未给出，需要参考相关资料）

## MPI 派生数据类型
- **预定义数据类型**：MPI_CHAR、MPI_INT、MPI_FLOAT、MPI_DOUBLE 等。
- **用户自定义数据类型**：
    - **优势**：
        - 允许使用单个 MPI 调用通信非连续数据，或不同数据类型。
        - 使代码更紧凑、易于维护。
        - 使非连续数据的通信更高效（取决于实现）。
    - **定义新数据类型**：
        1. 声明一个名为 MPI_Datatype 的新数据类型。
        2. 使用 MPI 提供的构造函数构建数据类型。
        3. 提交数据类型。
        4. 不再需要时，释放数据类型。
    - **常用构造函数**：
        - MPI_Type_contiguous()：创建连续数据类型。
        - MPI_Type_vector()：创建向量数据类型。
        - MPI_Type_indexed()：创建索引数据类型。
        - MPI_Type_create_subarray()：创建子数组数据类型。
        - MPI_Type_struct()：创建结构数据类型。

## MPI 笛卡尔拓扑例程
- **功能**：MPI 允许程序员将处理器组织成逻辑 k 维网格。
- **映射**：MPI_COMM_WORLD 中的进程 id 可以以多种方式映射到其他通信器（对应于更高维的网格）。
- **映射的好坏**：由底层程序的交互模式和机器的拓扑结构决定。

## 实验室练习
- **练习 1**：
    - 修改之前的程序，使用列分块循环划分对二维矩阵进行划分和分布。
    - 只有进程 0 创建一个完整的矩阵 A，所有进程创建一个矩阵 AK，仅包含分配的列块。
    - 矩阵分布完成后，所有进程将首先将其进程 id + 1 写入接收到的列块的所有条目，然后将块发送回进程 0。最后，所有块将回到原来的位置。
- **练习 2**：
    - 考虑并行块高斯消元法，在每次迭代中，一个列块的乘数从一个进程广播到所有其他进程，以更新尾随子矩阵。
    - 每个进程首先创建一个大小为 M x b 的工作二维数组 AW 并将其初始化为零。这个工作数组用于接收广播的数据元素。
    - 然后进程依次轮流广播 AK 中的列块，一次一个块。每次广播后，块的行数减少 b 行。
- **练习 3**：
    - 考虑并行块高斯消元法，在每次迭代中，小 b x b 子矩阵的下三角中的乘数从一个进程广播到所有其他进程，以更新领先行块。
    - 修改练习 4 中的程序，让进程依次轮流广播 AK 中每个列块顶部的小 b x b 子矩阵的下三角元素，一次一个。每次广播后，下一个列块的行长度减少 b。

## 总结
- **分布式内存平台**：
    - 每个进程都有自己的本地寻址空间。
    - 本地变量不能被其他进程访问（没有共享变量）。
    - 没有所有进程都可以访问的全局变量。
    - 要交换数据，进程必须明确地使用（变体）发送和接收消息传递机制进行通信。
- **数据和任务划分与分配**：
    - 在分布式内存机器上，数据分布在各个进程之间。
    - 需要同时考虑数据和任务划分与分配。
    - 对于数据/任务分配的限制更多，必须认真考虑数据局部性、负载平衡和资源利用效率。
    - 额外的数据通信成本，必须认真考虑如何最小化通信开销。

